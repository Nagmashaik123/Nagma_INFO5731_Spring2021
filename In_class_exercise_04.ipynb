{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "In_class_exercise_04.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nagmashaik123/Nagma_INFO5731_Spring2021/blob/main/In_class_exercise_04.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EuX00KHNeSpw"
      },
      "source": [
        "# **The fourth in-class-exercise (20 points in total, 2/9/2021)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s-vTOb03hG1f"
      },
      "source": [
        "# 1. Text Data Preprocessing\n",
        "\n",
        "Here is a [legal case](https://github.com/unt-iialab/info5731_spring2021/blob/main/class_exercises/01-05-1%20%20Adams%20v%20Tanner.txt) we collected from westlaw, please follow the steps we mentioned in lesson 5 to clean the data:\n",
        "\n",
        "\n",
        "\n",
        "## 1.1 Basic feature extraction using text data (4 points)\n",
        "\n",
        "*   Number of sentences\n",
        "*   Number of words\n",
        "*   Number of characters\n",
        "*   Average word length\n",
        "*   Number of stopwords\n",
        "*   Number of special characters\n",
        "*   Number of numerics\n",
        "*   Number of uppercase words\n",
        "\n",
        "## 1.2 Basic Text Pre-processing of text data (4 points)\n",
        "\n",
        "*   Lower casing\n",
        "*   Punctuation removal\n",
        "*   Stopwords removal\n",
        "*   Frequent words removal\n",
        "*   Rare words removal\n",
        "*   Spelling correction\n",
        "*   Tokenization\n",
        "*   Stemming\n",
        "*   Lemmatization\n",
        "\n",
        "## 1.3 Save all the **clean sentences** to a **csv file** (one column, each raw is a sentence) after finishing all the steps above. (4 points)\n",
        "\n",
        "\n",
        "## 1.4 Advance Text Processing (Extra credit: 4 points)\n",
        "\n",
        "*   Calculate the term frequency of all the terms.\n",
        "*   Print out top 10 1-gram, top 10 2-grams, and top 10 3-grams terms as features.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "muMsjoatp2XJ"
      },
      "source": [
        "1.1  Basic feature extraction using text data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vR0L3_CreM_A",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 643
        },
        "outputId": "446b3068-de1c-47be-89ec-e9fd8eea5938"
      },
      "source": [
        "# Program on text data preprocessing\n",
        "import pandas as pd\n",
        "from six.moves import urllib\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "\n",
        "array_data = []\n",
        "url_to_read = \"https://raw.githubusercontent.com/unt-iialab/info5731_spring2021/main/class_exercises/01-05-1%20%20Adams%20v%20Tanner.txt\"\n",
        "file_to_read = urllib.request.urlopen(url_to_read)\n",
        "\n",
        "for item in file_to_read:\n",
        "  modified_data = item.decode(\"utf-8\").replace('\\r\\n', '')\n",
        "  array_data.append(modified_data)\n",
        "\n",
        "while(\"\" in array_data) : \n",
        "    array_data.remove(\"\") \n",
        "    \n",
        "df = pd.DataFrame (array_data, columns = ['original_text'])\n",
        "\n",
        "\n",
        "# using pandas representation for columns\n",
        "df['review_sentence_count'] = df['original_text'].apply(lambda original_text:len(str(original_text).split(\".\")))\n",
        "df['review_words_count'] = df['original_text'].apply(lambda original_text: len(str(original_text).split(\" \")))\n",
        "df['review_char_count'] = df['original_text'].str.len()\n",
        "\n",
        "# calculation of word length.\n",
        "def avg_wrd_len_cal(column_statement):\n",
        "  words_per_line = column_statement.split()\n",
        "  if len(words_per_line) != 0:\n",
        "    return(sum(len(word) for word in words_per_line)/len(words_per_line))\n",
        "\n",
        "# avg word length column\n",
        "df['review_avg_word_len'] = df['original_text'].apply(lambda original_text: avg_wrd_len_cal(original_text))\n",
        "\n",
        "stop = stopwords.words('english')\n",
        "\n",
        "# stop words colum\n",
        "df['review_stop_words_count'] = df['original_text'].apply(lambda original_text: len([original_text for original_text in original_text.split() if original_text in stop]))\n",
        "df\n",
        "\n",
        "# no of special characters for each column\n",
        "def special_chars(col_stat):\n",
        "  chars = 0\n",
        "  for item in range(len(col_stat)):\n",
        "    if not((col_stat[item].isalpha()) and (col_stat[item].isdigit())):\n",
        "        chars = chars + 1\n",
        "  return chars\n",
        "\n",
        "df['review_special_char'] = df['original_text'].apply(lambda original_text: special_chars(original_text))\n",
        "\n",
        "# no of numerics \n",
        "df['review_no_numerics'] = df['original_text'].apply(lambda original_text: len([original_text for original_text in original_text.split() if original_text.isdigit()]))\n",
        "\n",
        "# no of upper case chars\n",
        "df['review_no_upper_words'] = df['original_text'].apply(lambda original_text: len([original_text for original_text in original_text.split() if original_text.isupper()]))\n",
        "\n",
        "df\n"
      ],
      "execution_count": 241,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>original_text</th>\n",
              "      <th>review_sentence_count</th>\n",
              "      <th>review_words_count</th>\n",
              "      <th>review_char_count</th>\n",
              "      <th>review_avg_word_len</th>\n",
              "      <th>review_stop_words_count</th>\n",
              "      <th>review_special_char</th>\n",
              "      <th>review_no_numerics</th>\n",
              "      <th>review_no_upper_words</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5 Ala. 740</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>10</td>\n",
              "      <td>2.666667</td>\n",
              "      <td>0</td>\n",
              "      <td>10</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Supreme Court of Alabama.</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>25</td>\n",
              "      <td>5.500000</td>\n",
              "      <td>1</td>\n",
              "      <td>25</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>ADAMS</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>v.</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>TANNER AND HORTON.</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>18</td>\n",
              "      <td>5.333333</td>\n",
              "      <td>0</td>\n",
              "      <td>18</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>143</th>\n",
              "      <td>There are no Filings for this citation.</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>39</td>\n",
              "      <td>4.714286</td>\n",
              "      <td>4</td>\n",
              "      <td>39</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>144</th>\n",
              "      <td>Negative Treatment</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>18</td>\n",
              "      <td>8.500000</td>\n",
              "      <td>0</td>\n",
              "      <td>18</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>145</th>\n",
              "      <td>There are no Negative Treatment results for th...</td>\n",
              "      <td>2</td>\n",
              "      <td>9</td>\n",
              "      <td>58</td>\n",
              "      <td>5.555556</td>\n",
              "      <td>4</td>\n",
              "      <td>58</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>146</th>\n",
              "      <td>History</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>7.000000</td>\n",
              "      <td>0</td>\n",
              "      <td>7</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>147</th>\n",
              "      <td>There are no History results for this citation.</td>\n",
              "      <td>2</td>\n",
              "      <td>8</td>\n",
              "      <td>47</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>4</td>\n",
              "      <td>47</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>148 rows × 9 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                         original_text  ...  review_no_upper_words\n",
              "0                                           5 Ala. 740  ...                      0\n",
              "1                            Supreme Court of Alabama.  ...                      0\n",
              "2                                                ADAMS  ...                      1\n",
              "3                                                   v.  ...                      0\n",
              "4                                   TANNER AND HORTON.  ...                      3\n",
              "..                                                 ...  ...                    ...\n",
              "143            There are no Filings for this citation.  ...                      0\n",
              "144                                 Negative Treatment  ...                      0\n",
              "145  There are no Negative Treatment results for th...  ...                      0\n",
              "146                                            History  ...                      0\n",
              "147    There are no History results for this citation.  ...                      0\n",
              "\n",
              "[148 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 241
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YVe4NXdNgCI8",
        "outputId": "ed886de7-4b25-4ebd-8341-1bf9fe29843c"
      },
      "source": [
        "# Returning the count of sentences , words and characters\n",
        "\n",
        "sentence_count = df['review_sentence_count'].sum()\n",
        "word_count = df['review_words_count'].sum()\n",
        "char_count = df['review_char_count'].sum()\n",
        "word_len = df['review_avg_word_len'].sum()\n",
        "stop_words = df['review_stop_words_count'].sum()\n",
        "special_chars = df['review_special_char'].sum()\n",
        "no_numerics = df['review_no_numerics'].sum()\n",
        "no_upper_words = df['review_no_upper_words'].sum()\n",
        "\n",
        "\n",
        "\n",
        "print(\"Total no sentences : \" , sentence_count) \n",
        "print(\"Total no words : \" , word_count)\n",
        "print(\"Total no characters : \" , char_count)\n",
        "print(\"Total avg word length : \" , word_len)\n",
        "print(\"Total no of stop words : \" , stop_words)\n",
        "print(\"Total no of special chars : \" , special_chars)\n",
        "print(\"Total no of numerics : \" , no_numerics)\n",
        "print(\"Total no of upper words : \", no_upper_words)\n",
        "\n"
      ],
      "execution_count": 243,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total no sentences :  439\n",
            "Total no words :  3722\n",
            "Total no characters :  20295\n",
            "Total avg word length :  642.1036748280112\n",
            "Total no of stop words :  1679\n",
            "Total no of special chars :  20295\n",
            "Total no of numerics :  60\n",
            "Total no of upper words :  84\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nxb_IOKGrqzQ"
      },
      "source": [
        "1.2 Basic Text Pre-processing of text data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "aHkiYZ4sr2PF",
        "outputId": "a38a54f1-c69d-46f8-d9b0-d33d921dd0e0"
      },
      "source": [
        "\n",
        "import nltk\n",
        "from textblob import Word\n",
        "from nltk.corpus import stopwords\n",
        "from textblob import TextBlob\n",
        "from nltk.stem import PorterStemmer\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "stop = stopwords.words('english')\n",
        "\n",
        "df['convert_lower_case'] = df['original_text'].apply(lambda original_text: \" \".join(original_text.lower() for original_text in original_text.split()))\n",
        "df['punctuation_removal'] = df['original_text'].str.replace('[^\\w\\s]','')\n",
        "df['Stopwords_removal'] = df['punctuation_removal'].apply(lambda original_text: \" \".join(original_text for original_text in original_text.split() if original_text not in stop))\n",
        "\n",
        "dup_words = pd.Series(' '.join(df['Stopwords_removal']).split()).value_counts()[:10]\n",
        "print(\"Duplicate words to be removed : \\n\",dup_words)\n",
        "dup_words_list = list(dup_words.index)\n",
        "\n",
        "# removing frequency words\n",
        "df['frequent_words_removal'] = df['Stopwords_removal'].apply(lambda original_text: \" \".join(original_text for original_text in original_text.split() if original_text not in dup_words_list))\n",
        "\n",
        "# removing rare words \n",
        "rare_words = pd.Series(' '.join(df['Stopwords_removal']).split()).value_counts()[-10:]\n",
        "print(\"Rare words to be removed : \\n\",rare_words)\n",
        "rare_words_list = list(rare_words.index)\n",
        "df['Rare_words_removal'] = df['frequent_words_removal'].apply(lambda original_text: \" \".join(original_text for original_text in original_text.split() if original_text not in rare_words_list))\n",
        "\n",
        "# Special correction\n",
        "df['Spelling_correction'] = df['Rare_words_removal'].apply(lambda original_text: str(TextBlob(original_text).correct()))\n",
        "\n",
        "# Tokenization \n",
        "df['Tokenization'] = df['Spelling_correction'].apply(lambda original_text: TextBlob(original_text).words)\n",
        "\n",
        "# Stemming\n",
        "st = PorterStemmer()\n",
        "df['Stemming'] = df['Tokenization'].apply(lambda original_text: \" \".join([st.stem(word) for word in original_text]))\n",
        "\n",
        "df['Lemmatization'] = df['Stemming'].apply(lambda original_text: \" \".join([Word(word).lemmatize() for word in original_text.split()]))\n",
        "df\n",
        "\n"
      ],
      "execution_count": 244,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "Duplicate words to be removed : \n",
            " execution    50\n",
            "crop         49\n",
            "The          28\n",
            "levy         25\n",
            "lien         24\n",
            "claimants    22\n",
            "v            22\n",
            "right        20\n",
            "gathered     19\n",
            "contract     16\n",
            "dtype: int64\n",
            "Rare words to be removed : \n",
            " temporary        1\n",
            "1857             1\n",
            "108              1\n",
            "retrospective    1\n",
            "cultivated       1\n",
            "1828             1\n",
            "bona             1\n",
            "Clays            1\n",
            "Attorneys        1\n",
            "West             1\n",
            "dtype: int64\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>original_text</th>\n",
              "      <th>review_sentence_count</th>\n",
              "      <th>review_words_count</th>\n",
              "      <th>review_char_count</th>\n",
              "      <th>review_avg_word_len</th>\n",
              "      <th>review_stop_words_count</th>\n",
              "      <th>review_special_char</th>\n",
              "      <th>review_no_numerics</th>\n",
              "      <th>review_no_upper_words</th>\n",
              "      <th>convert_lower_case</th>\n",
              "      <th>punctuation_removal</th>\n",
              "      <th>Stopwords_removal</th>\n",
              "      <th>frequent_words_removal</th>\n",
              "      <th>Rare_words_removal</th>\n",
              "      <th>Spelling_correction</th>\n",
              "      <th>Tokenization</th>\n",
              "      <th>Stemming</th>\n",
              "      <th>Lemmatization</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5 Ala. 740</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>10</td>\n",
              "      <td>2.666667</td>\n",
              "      <td>0</td>\n",
              "      <td>10</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>5 ala. 740</td>\n",
              "      <td>5 Ala 740</td>\n",
              "      <td>5 Ala 740</td>\n",
              "      <td>5 Ala 740</td>\n",
              "      <td>5 Ala 740</td>\n",
              "      <td>5 La 740</td>\n",
              "      <td>[5, La, 740]</td>\n",
              "      <td>5 La 740</td>\n",
              "      <td>5 La 740</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Supreme Court of Alabama.</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>25</td>\n",
              "      <td>5.500000</td>\n",
              "      <td>1</td>\n",
              "      <td>25</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>supreme court of alabama.</td>\n",
              "      <td>Supreme Court of Alabama</td>\n",
              "      <td>Supreme Court Alabama</td>\n",
              "      <td>Supreme Court Alabama</td>\n",
              "      <td>Supreme Court Alabama</td>\n",
              "      <td>Supreme Court Alabama</td>\n",
              "      <td>[Supreme, Court, Alabama]</td>\n",
              "      <td>suprem court alabama</td>\n",
              "      <td>suprem court alabama</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>ADAMS</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>adams</td>\n",
              "      <td>ADAMS</td>\n",
              "      <td>ADAMS</td>\n",
              "      <td>ADAMS</td>\n",
              "      <td>ADAMS</td>\n",
              "      <td>ADAMS</td>\n",
              "      <td>[ADAMS]</td>\n",
              "      <td>adam</td>\n",
              "      <td>adam</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>v.</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>v.</td>\n",
              "      <td>v</td>\n",
              "      <td>v</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>[]</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>TANNER AND HORTON.</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>18</td>\n",
              "      <td>5.333333</td>\n",
              "      <td>0</td>\n",
              "      <td>18</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>tanner and horton.</td>\n",
              "      <td>TANNER AND HORTON</td>\n",
              "      <td>TANNER AND HORTON</td>\n",
              "      <td>TANNER AND HORTON</td>\n",
              "      <td>TANNER AND HORTON</td>\n",
              "      <td>TANNER AND HORTON</td>\n",
              "      <td>[TANNER, AND, HORTON]</td>\n",
              "      <td>tanner and horton</td>\n",
              "      <td>tanner and horton</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>143</th>\n",
              "      <td>There are no Filings for this citation.</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>39</td>\n",
              "      <td>4.714286</td>\n",
              "      <td>4</td>\n",
              "      <td>39</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>there are no filings for this citation.</td>\n",
              "      <td>There are no Filings for this citation</td>\n",
              "      <td>There Filings citation</td>\n",
              "      <td>There Filings citation</td>\n",
              "      <td>There Filings citation</td>\n",
              "      <td>There Tidings situation</td>\n",
              "      <td>[There, Tidings, situation]</td>\n",
              "      <td>there tide situat</td>\n",
              "      <td>there tide situat</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>144</th>\n",
              "      <td>Negative Treatment</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>18</td>\n",
              "      <td>8.500000</td>\n",
              "      <td>0</td>\n",
              "      <td>18</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>negative treatment</td>\n",
              "      <td>Negative Treatment</td>\n",
              "      <td>Negative Treatment</td>\n",
              "      <td>Negative Treatment</td>\n",
              "      <td>Negative Treatment</td>\n",
              "      <td>Negative Treatment</td>\n",
              "      <td>[Negative, Treatment]</td>\n",
              "      <td>neg treatment</td>\n",
              "      <td>neg treatment</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>145</th>\n",
              "      <td>There are no Negative Treatment results for th...</td>\n",
              "      <td>2</td>\n",
              "      <td>9</td>\n",
              "      <td>58</td>\n",
              "      <td>5.555556</td>\n",
              "      <td>4</td>\n",
              "      <td>58</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>there are no negative treatment results for th...</td>\n",
              "      <td>There are no Negative Treatment results for th...</td>\n",
              "      <td>There Negative Treatment results citation</td>\n",
              "      <td>There Negative Treatment results citation</td>\n",
              "      <td>There Negative Treatment results citation</td>\n",
              "      <td>There Negative Treatment results situation</td>\n",
              "      <td>[There, Negative, Treatment, results, situation]</td>\n",
              "      <td>there neg treatment result situat</td>\n",
              "      <td>there neg treatment result situat</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>146</th>\n",
              "      <td>History</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>7.000000</td>\n",
              "      <td>0</td>\n",
              "      <td>7</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>history</td>\n",
              "      <td>History</td>\n",
              "      <td>History</td>\n",
              "      <td>History</td>\n",
              "      <td>History</td>\n",
              "      <td>History</td>\n",
              "      <td>[History]</td>\n",
              "      <td>histori</td>\n",
              "      <td>histori</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>147</th>\n",
              "      <td>There are no History results for this citation.</td>\n",
              "      <td>2</td>\n",
              "      <td>8</td>\n",
              "      <td>47</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>4</td>\n",
              "      <td>47</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>there are no history results for this citation.</td>\n",
              "      <td>There are no History results for this citation</td>\n",
              "      <td>There History results citation</td>\n",
              "      <td>There History results citation</td>\n",
              "      <td>There History results citation</td>\n",
              "      <td>There History results situation</td>\n",
              "      <td>[There, History, results, situation]</td>\n",
              "      <td>there histori result situat</td>\n",
              "      <td>there histori result situat</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>148 rows × 18 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                         original_text  ...                      Lemmatization\n",
              "0                                           5 Ala. 740  ...                           5 La 740\n",
              "1                            Supreme Court of Alabama.  ...               suprem court alabama\n",
              "2                                                ADAMS  ...                               adam\n",
              "3                                                   v.  ...                                   \n",
              "4                                   TANNER AND HORTON.  ...                  tanner and horton\n",
              "..                                                 ...  ...                                ...\n",
              "143            There are no Filings for this citation.  ...                  there tide situat\n",
              "144                                 Negative Treatment  ...                      neg treatment\n",
              "145  There are no Negative Treatment results for th...  ...  there neg treatment result situat\n",
              "146                                            History  ...                            histori\n",
              "147    There are no History results for this citation.  ...        there histori result situat\n",
              "\n",
              "[148 rows x 18 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 244
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "__dYwz4NyzRT"
      },
      "source": [
        "# Saving data to CSV file\n",
        "df.to_csv('final_data.csv',index=False)"
      ],
      "execution_count": 245,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "b829VGt7zJF7",
        "outputId": "81328a2b-7187-4482-c029-d83f84c2cfbe"
      },
      "source": [
        "#1.4 Advance Text Processing\n",
        "\n",
        "tf1 = (df['Lemmatization']).apply(lambda original_text: pd.value_counts(original_text.split(\" \"))).sum(axis = 0).reset_index()\n",
        "tf1.columns = ['words_list','term_frequency']\n",
        "tf1"
      ],
      "execution_count": 246,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>words_list</th>\n",
              "      <th>term_frequency</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5</td>\n",
              "      <td>8.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>740</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>La</td>\n",
              "      <td>15.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>court</td>\n",
              "      <td>20.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>alabama</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>725</th>\n",
              "      <td>yield</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>726</th>\n",
              "      <td>rye</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>727</th>\n",
              "      <td>tide</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>728</th>\n",
              "      <td>neg</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>729</th>\n",
              "      <td>histori</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>730 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "    words_list  term_frequency\n",
              "0            5             8.0\n",
              "1          740             2.0\n",
              "2           La            15.0\n",
              "3        court            20.0\n",
              "4      alabama             1.0\n",
              "..         ...             ...\n",
              "725      yield             1.0\n",
              "726        rye             1.0\n",
              "727       tide             2.0\n",
              "728        neg             2.0\n",
              "729    histori             2.0\n",
              "\n",
              "[730 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 246
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k5yFs4lrzxbU",
        "outputId": "cd712fe6-6040-424a-b002-7ac7b7ae5eef"
      },
      "source": [
        "# Top 10 - one grams\n",
        "import itertools\n",
        "import collections\n",
        "from nltk import ngrams\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "word_array = []\n",
        "for item in df['Lemmatization']:\n",
        "  word_array.append(word_tokenize(item))\n",
        "dataProcessed = [x for x in word_array if x != []]\n",
        "cycleData = list(itertools.chain.from_iterable(dataProcessed))\n",
        "oneGrams = ngrams(cycleData, 1)\n",
        "collections.Counter(oneGrams).most_common(10)\n"
      ],
      "execution_count": 247,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(('case',), 24),\n",
              " (('court',), 20),\n",
              " (('grow',), 17),\n",
              " (('law',), 17),\n",
              " (('rep',), 16),\n",
              " (('La',), 15),\n",
              " (('plaintiff',), 15),\n",
              " (('posse',), 14),\n",
              " (('attach',), 14),\n",
              " (('cotton',), 14)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 247
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BBiC4E_kefvV"
      },
      "source": [
        "# 2. Python Regular Expression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z1QJ-UwCenvN"
      },
      "source": [
        "## 2.1 Write a Python program to remove leading zeros from an IP address. (4 points)\n",
        "\n",
        "ip = \"260.08.094.109\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wSv6fVhOfFmv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c7c8a3b-9da8-4eb6-ea76-487a08d10a3d"
      },
      "source": [
        "# Progran to remove leading zeros from IP address\n",
        "import re\n",
        "ip =  \"260.08.094.109\"\n",
        "address_with_out_zero = re.sub('\\.[0]*', '.', ip)\n",
        "print(address_with_out_zero)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 248,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "260.8.94.109\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PXRjaHzrfKAy"
      },
      "source": [
        "## 2.2 Write a Python Program to extract all the years from the following sentence. (4 points)\n",
        "\n",
        "sentence = \"The 2010s were a dramatic decade, filled with ups and downs, more than 1000 stroies have happened. As the decade comes to a close, Insider took a look back at some of the biggest headline-grabbing stories, from 2010 to 2019. The result was 119 news stories that ranged from the heartwarming rescue of a Thai boys' soccer team from a flooded cave to the divisive election of President Donald Trump.\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7xdJpDx9gjbX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef3a8a39-acf6-441a-f080-50d7787f250a"
      },
      "source": [
        "# Program to extract all the years\n",
        "import re\n",
        "sentence = \"The 2010s were a dramatic decade, filled with ups and downs, more than 1000 stroies have happened. As the decade comes to a close, Insider took a look back at some of the biggest headline-grabbing stories, from 2010 to 2019. The result was 119 news stories that ranged from the heartwarming rescue of a Thai boys' soccer team from a flooded cave to the divisive election of President Donald Trump.\"\n",
        "years_extracted = re.findall(r'2\\d\\d\\d', sentence)\n",
        "print(years_extracted)"
      ],
      "execution_count": 249,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['2010', '2010', '2019']\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}